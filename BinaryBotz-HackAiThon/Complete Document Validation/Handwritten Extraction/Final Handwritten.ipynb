{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19928f12-8319-446b-b895-26be6cbd1aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 1024,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-large-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 D:\\SBI Life Hack-AI-Thon\\Handwritten Extraction\\Post_Mortem.jpg: 640x608 136 0s, 70.5ms\n",
      "Speed: 3.0ms preprocess, 70.5ms inference, 76.5ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Extracted Paragraph:\n",
      "a )8 (24 .\n",
      "What Benson Ferorted . You\n",
      "R. G. KAR MEDICAL COLLEGE & HOSPITAL , KOLKATA\n",
      "Date # case and symptoms # Prescription # Die\n",
      "Immediate demands :\n",
      "Enquiry and Post mortem to too conducted .\n",
      "under supervision of judicial magistrate\n",
      "and Judicial magistrate should be frequent\n",
      "imagistroare\n",
      "air the time of boat roofism .\n",
      "video Rewading should be done frompushsory )\n",
      "d\"from Representative\n",
      "2 . Post mortem should be done under\n",
      "supervision of a broad formed , composting of : \"\n",
      "a multiple senior Autopsy subgeons .\n",
      "# Preference of \" Female Faculty from\n",
      "Foregressive Department\n",
      "a also involve faculty of topestic\n",
      "Department of the other institute .\n",
      "of poor mom should be done by today ( a jafas )\n",
      "evening .\n",
      "A post mortum done under different a senior\n",
      "revisents , on spot of Autopsy .\n",
      "from All the residents of\n",
      "e.m. Turn musical collapse\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# -----------------------------\n",
    "# Set up device for GPU usage\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Load models\n",
    "# YOLO (for word detection) â€“ note: YOLO does not support .to(device)\n",
    "yolo_model = YOLO(\"./word_Detection.pt\")\n",
    "# TrOCR (for OCR on line images)\n",
    "trocr_processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-large-handwritten\")\n",
    "trocr_model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-large-handwritten\").to(device)\n",
    "\n",
    "# -----------------------------\n",
    "# Load the input image\n",
    "image_path = \"./Post_Mortem.jpg\"  # update with your image path\n",
    "original_image = cv2.imread(image_path)\n",
    "if original_image is None:\n",
    "    raise ValueError(\"Could not load image. Check the file path.\")\n",
    "h, w, _ = original_image.shape\n",
    "\n",
    "# -----------------------------\n",
    "# Run YOLO inference to detect words\n",
    "results = yolo_model(image_path)\n",
    "detections = results[0].boxes  # Assumes detections are stored here\n",
    "\n",
    "# Collect each detected word's bounding box\n",
    "word_boxes = []  # Each entry: (x_min, y_min, x_max, y_max)\n",
    "for box in detections:\n",
    "    xyxy = box.xyxy[0].tolist()\n",
    "    x_min, y_min, x_max, y_max = map(int, xyxy)\n",
    "    # Ensure coordinates are within image bounds\n",
    "    x_min = max(0, x_min)\n",
    "    y_min = max(0, y_min)\n",
    "    x_max = min(w, x_max)\n",
    "    y_max = min(h, y_max)\n",
    "    word_boxes.append((x_min, y_min, x_max, y_max))\n",
    "\n",
    "# -----------------------------\n",
    "# Dynamically compute vertical threshold based on average word height\n",
    "heights = [b[3] - b[1] for b in word_boxes]\n",
    "avg_height = np.mean(heights) if heights else 20\n",
    "vertical_threshold = avg_height * 0.6  # Adjust multiplier as needed\n",
    "\n",
    "# -----------------------------\n",
    "# Group word boxes into lines based on vertical proximity\n",
    "lines = []  # Each element will be a list of word boxes\n",
    "for box in word_boxes:\n",
    "    x_min, y_min, x_max, y_max = box\n",
    "    center_y = (y_min + y_max) / 2\n",
    "    added = False\n",
    "    # Try to add the box to an existing line group\n",
    "    for line in lines:\n",
    "        line_centers = [ (b[1] + b[3]) / 2 for b in line ]\n",
    "        avg_center = sum(line_centers) / len(line_centers)\n",
    "        if abs(center_y - avg_center) < vertical_threshold:\n",
    "            line.append(box)\n",
    "            added = True\n",
    "            break\n",
    "    if not added:\n",
    "        lines.append([box])\n",
    "\n",
    "# For each line, sort the boxes from left to right\n",
    "for i in range(len(lines)):\n",
    "    lines[i].sort(key=lambda b: b[0])\n",
    "\n",
    "# -----------------------------\n",
    "# Merge word boxes in each line and run TrOCR to extract text\n",
    "line_boxes = []  # Each entry: (x_min, y_min, x_max, y_max, line_text)\n",
    "for line in lines:\n",
    "    # Merge: union of all boxes in the line\n",
    "    x_min_line = min(b[0] for b in line)\n",
    "    y_min_line = min(b[1] for b in line)\n",
    "    x_max_line = max(b[2] for b in line)\n",
    "    y_max_line = max(b[3] for b in line)\n",
    "    \n",
    "    # Crop the merged region from the original image\n",
    "    line_region = original_image[y_min_line:y_max_line, x_min_line:x_max_line]\n",
    "    line_region_pil = Image.fromarray(cv2.cvtColor(line_region, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    # Run TrOCR on the line image\n",
    "    pixel_values = trocr_processor(images=line_region_pil, return_tensors=\"pt\").pixel_values.to(device)\n",
    "    with torch.no_grad():\n",
    "        generated_ids = trocr_model.generate(pixel_values)\n",
    "    line_text = trocr_processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "    \n",
    "    line_boxes.append((x_min_line, y_min_line, x_max_line, y_max_line, line_text))\n",
    "\n",
    "# -----------------------------\n",
    "# Sort the lines in reading order (top to bottom)\n",
    "line_boxes.sort(key=lambda x: x[1])\n",
    "\n",
    "# -----------------------------\n",
    "# Post-process: Remove duplicate words between consecutive lines\n",
    "clean_lines = []\n",
    "for i, (_, _, _, _, line_text) in enumerate(line_boxes):\n",
    "    words = line_text.split()\n",
    "    if i > 0 and clean_lines:\n",
    "        prev_words = clean_lines[-1].split()\n",
    "        # If the last word of the previous line equals the first word of the current line (case-insensitive), remove it.\n",
    "        if prev_words and words and prev_words[-1].lower() == words[0].lower():\n",
    "            words = words[1:]\n",
    "    clean_lines.append(\" \".join(words))\n",
    "\n",
    "# Combine the cleaned lines into a final paragraph\n",
    "final_text = \"\\n\".join(clean_lines)\n",
    "\n",
    "print(\"Extracted Paragraph:\")\n",
    "print(final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8432b875-0025-46ce-a77e-c83287f003c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
